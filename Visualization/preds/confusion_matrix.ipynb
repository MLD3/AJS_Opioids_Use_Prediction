{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics, utils\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm#_notebook as tqdm\n",
    "import scipy\n",
    "import scipy.special\n",
    "import itertools\n",
    "import yaml\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ys(model_name, outcome):\n",
    "    y_true = np.load(path + outcome + '_test.npy')\n",
    "    y_score = None\n",
    "    if model_name == 'xgb' or model_name == 'linear_svc':\n",
    "        y_score = np.load(path + model_name + '_' + outcome + '_y_score.npy')\n",
    "    else:\n",
    "        y_score = np.load(path + model_name + '_' + outcome + '_y_score.npz')['y_score']      \n",
    "    return y_true, y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostrap_func_all(i, y_true, y_prob, threshold):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_prob)\n",
    "    y_true_b, y_prob_b = utils.resample(y_true, y_prob, replace=True, random_state=i)\n",
    "    y_pred_b = (y_prob_b > threshold)\n",
    "    tpr_cutoff = metrics.recall_score(y_true_b, y_pred_b)\n",
    "    idx = (np.abs(tpr - tpr_cutoff)).argmin()\n",
    "    \n",
    "    return (\n",
    "        metrics.roc_auc_score(y_true_b, y_prob_b), # AUC\n",
    "        tpr[idx], # sensitivity\n",
    "        1-fpr[idx], # specificity\n",
    "        metrics.precision_score(y_true_b, y_pred_b), # positive predictive value\n",
    "    )\n",
    "\n",
    "def boostrap_func_confusion(i, y_true, y_prob, threshold):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_prob)\n",
    "    y_true_b, y_prob_b = utils.resample(y_true, y_prob, replace=True, random_state=i)\n",
    "    y_pred_b = (y_prob_b > threshold)\n",
    "    tpr_cutoff = metrics.recall_score(y_true_b, y_pred_b)\n",
    "    idx = (np.abs(tpr - tpr_cutoff)).argmin()\n",
    "    \n",
    "    return metrics.confusion_matrix(y_true_b, y_pred_b).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'linear_svc'\n",
    "task = 'refill'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh (18228, -0.6528159803079673) Prec=0.200  Rec=0.360  Spec=0.833\n",
      "scores (95%CI lower, upper)\n",
      "Test AUC 0.666 (0.654, 0.678)\n",
      "sens.\t 36.0% (34.2%, 38.0%)\n",
      "spec.\t 83.2% (82.0%, 84.1%)\n",
      "prec.\t 20.0% (18.8%, 21.2%)\n"
     ]
    }
   ],
   "source": [
    "y_true, y_score = get_ys(model, task)\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_true, y_score)\n",
    "prec, rec, thresholds = metrics.precision_recall_curve(y_true, y_score)\n",
    "\n",
    "max_rec = rec[prec >= 0.2].max()\n",
    "select_thresh = thresholds[prec[:-1] >= 0.2][rec[:-1][prec[:-1] >= 0.2].argmax()]\n",
    "select_idx = np.argmin(np.abs(thresholds-select_thresh))\n",
    "select_prec = prec[select_idx]\n",
    "select_spec = 1 - fpr[np.argmin(np.abs(tpr-max_rec))]\n",
    "print('Thresh', (select_idx, select_thresh), 'Prec={:.3f}'.format(select_prec), ' Rec={:.3f}'.format(max_rec), ' Spec={:.3f}'.format(select_spec))\n",
    "\n",
    "print('scores (95%CI lower, upper)')\n",
    "auc_scores, sensitivities, specificities, ppvs = zip(*Parallel(n_jobs=4)(delayed(boostrap_func_all)(i, y_true, y_score, select_thresh) for i in range(1000)))\n",
    "print('Test AUC {:.3f} ({:.3f}, {:.3f})'.format(np.mean(auc_scores), np.percentile(auc_scores, 2.5), np.percentile(auc_scores, 97.5)))\n",
    "print('sens.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(sensitivities), np.percentile(sensitivities, 2.5), np.percentile(sensitivities, 97.5)))\n",
    "print('spec.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(specificities), np.percentile(specificities, 2.5), np.percentile(specificities, 97.5)))\n",
    "print('prec.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(ppvs), np.percentile(ppvs, 2.5), np.percentile(ppvs, 97.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'xgb'\n",
    "task = 'refill'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh (16919, 0.25270462) Prec=0.200  Rec=0.464  Spec=0.785\n",
      "scores (95%CI lower, upper)\n",
      "Test AUC 0.681 (0.669, 0.693)\n",
      "sens.\t 46.5% (44.5%, 48.5%)\n",
      "spec.\t 78.3% (76.5%, 79.8%)\n",
      "prec.\t 20.0% (19.0%, 21.1%)\n"
     ]
    }
   ],
   "source": [
    "y_true, y_score = get_ys(model, task)\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_true, y_score)\n",
    "prec, rec, thresholds = metrics.precision_recall_curve(y_true, y_score)\n",
    "\n",
    "max_rec = rec[prec >= 0.2].max()\n",
    "select_thresh = thresholds[prec[:-1] >= 0.2][rec[:-1][prec[:-1] >= 0.2].argmax()]\n",
    "select_idx = np.argmin(np.abs(thresholds-select_thresh))\n",
    "select_prec = prec[select_idx]\n",
    "select_spec = 1 - fpr[np.argmin(np.abs(tpr-max_rec))]\n",
    "print('Thresh', (select_idx, select_thresh), 'Prec={:.3f}'.format(select_prec), ' Rec={:.3f}'.format(max_rec), ' Spec={:.3f}'.format(select_spec))\n",
    "\n",
    "print('scores (95%CI lower, upper)')\n",
    "auc_scores, sensitivities, specificities, ppvs = zip(*Parallel(n_jobs=4)(delayed(boostrap_func_all)(i, y_true, y_score, select_thresh) for i in range(1000)))\n",
    "print('Test AUC {:.3f} ({:.3f}, {:.3f})'.format(np.mean(auc_scores), np.percentile(auc_scores, 2.5), np.percentile(auc_scores, 97.5)))\n",
    "print('sens.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(sensitivities), np.percentile(sensitivities, 2.5), np.percentile(sensitivities, 97.5)))\n",
    "print('spec.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(specificities), np.percentile(specificities, 2.5), np.percentile(specificities, 97.5)))\n",
    "print('prec.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(ppvs), np.percentile(ppvs, 2.5), np.percentile(ppvs, 97.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'linear_svc'\n",
    "task = 'prolonged_use'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh (22280, -0.6726043713977421) Prec=0.200  Rec=0.042  Spec=0.991\n",
      "scores (95%CI lower, upper)\n",
      "Test AUC 0.660 (0.643, 0.675)\n",
      "sens.\t 4.1% (3.0%, 5.4%)\n",
      "spec.\t 99.1% (98.7%, 99.4%)\n",
      "prec.\t 20.1% (15.1%, 25.4%)\n"
     ]
    }
   ],
   "source": [
    "y_true, y_score = get_ys(model, task)\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_true, y_score)\n",
    "prec, rec, thresholds = metrics.precision_recall_curve(y_true, y_score)\n",
    "\n",
    "max_rec = rec[prec >= 0.2].max()\n",
    "select_thresh = thresholds[prec[:-1] >= 0.2][rec[:-1][prec[:-1] >= 0.2].argmax()]\n",
    "select_idx = np.argmin(np.abs(thresholds-select_thresh))\n",
    "select_prec = prec[select_idx]\n",
    "select_spec = 1 - fpr[np.argmin(np.abs(tpr-max_rec))]\n",
    "print('Thresh', (select_idx, select_thresh), 'Prec={:.3f}'.format(select_prec), ' Rec={:.3f}'.format(max_rec), ' Spec={:.3f}'.format(select_spec))\n",
    "\n",
    "print('scores (95%CI lower, upper)')\n",
    "auc_scores, sensitivities, specificities, ppvs = zip(*Parallel(n_jobs=4)(delayed(boostrap_func_all)(i, y_true, y_score, select_thresh) for i in range(1000)))\n",
    "print('Test AUC {:.3f} ({:.3f}, {:.3f})'.format(np.mean(auc_scores), np.percentile(auc_scores, 2.5), np.percentile(auc_scores, 97.5)))\n",
    "print('sens.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(sensitivities), np.percentile(sensitivities, 2.5), np.percentile(sensitivities, 97.5)))\n",
    "print('spec.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(specificities), np.percentile(specificities, 2.5), np.percentile(specificities, 97.5)))\n",
    "print('prec.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(ppvs), np.percentile(ppvs, 2.5), np.percentile(ppvs, 97.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'xgb'\n",
    "task = 'prolonged_use'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh (22017, 0.20804794) Prec=0.200  Rec=0.036  Spec=0.992\n",
      "scores (95%CI lower, upper)\n",
      "Test AUC 0.659 (0.642, 0.675)\n",
      "sens.\t 3.6% (2.6%, 4.7%)\n",
      "spec.\t 99.2% (98.9%, 99.5%)\n",
      "prec.\t 20.0% (15.0%, 25.6%)\n"
     ]
    }
   ],
   "source": [
    "y_true, y_score = get_ys(model, task)\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_true, y_score)\n",
    "prec, rec, thresholds = metrics.precision_recall_curve(y_true, y_score)\n",
    "\n",
    "max_rec = rec[prec >= 0.2].max()\n",
    "select_thresh = thresholds[prec[:-1] >= 0.2][rec[:-1][prec[:-1] >= 0.2].argmax()]\n",
    "select_idx = np.argmin(np.abs(thresholds-select_thresh))\n",
    "select_prec = prec[select_idx]\n",
    "select_spec = 1 - fpr[np.argmin(np.abs(tpr-max_rec))]\n",
    "print('Thresh', (select_idx, select_thresh), 'Prec={:.3f}'.format(select_prec), ' Rec={:.3f}'.format(max_rec), ' Spec={:.3f}'.format(select_spec))\n",
    "\n",
    "print('scores (95%CI lower, upper)')\n",
    "auc_scores, sensitivities, specificities, ppvs = zip(*Parallel(n_jobs=4)(delayed(boostrap_func_all)(i, y_true, y_score, select_thresh) for i in range(1000)))\n",
    "print('Test AUC {:.3f} ({:.3f}, {:.3f})'.format(np.mean(auc_scores), np.percentile(auc_scores, 2.5), np.percentile(auc_scores, 97.5)))\n",
    "print('sens.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(sensitivities), np.percentile(sensitivities, 2.5), np.percentile(sensitivities, 97.5)))\n",
    "print('spec.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(specificities), np.percentile(specificities, 2.5), np.percentile(specificities, 97.5)))\n",
    "print('prec.\\t {:.1%} ({:.1%}, {:.1%})'.format(np.mean(ppvs), np.percentile(ppvs, 2.5), np.percentile(ppvs, 97.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
